{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegNet model implemented with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage import color\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras import models\n",
    "from tensorflow.contrib.keras.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.contrib.keras.python.keras.models import model_from_json\n",
    "from tensorflow.contrib.keras.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build model\n",
    "from tensorflow.contrib.keras.python.keras.layers.core import Activation, Reshape, Permute\n",
    "from tensorflow.contrib.keras.python.keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dropout\n",
    "from tensorflow.contrib.keras.python.keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = ''\n",
    "# img_w = 512\n",
    "# img_w = 424\n",
    "img_w = 128\n",
    "img_h = 128\n",
    "\n",
    "n_labels = 2\n",
    "\n",
    "kernel = 3\n",
    "\n",
    "n_train = 12250\n",
    "n_test = 5250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_map(labels):\n",
    "    label_map = np.zeros([img_h, img_w, n_labels])    \n",
    "    for r in range(img_h):\n",
    "        for c in range(img_w):\n",
    "            label_map[r, c, int(np.ceil(labels[r][c]))] = 1\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_data(mode):\n",
    "    assert mode in {'test', 'train'}, \\\n",
    "        'mode should be either \\'test\\' or \\'train\\''\n",
    "    data = []\n",
    "    label = []\n",
    "    df = pd.read_csv(path + mode + '.csv')\n",
    "    n = n_train if mode == 'train' else n_test\n",
    "    for i, item in df.iterrows():\n",
    "        if i >= n:\n",
    "            break\n",
    "        img, gt = [resize(imread(path + item[0]), (img_h, img_w))], resize(np.clip(~(imread(path + item[1])[:, :, 0]), 0, 1), (img_h, img_w))\n",
    "        \n",
    "        data.append(img)\n",
    "        label.append(label_map(gt))\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(mode + \": [%-20s] %d%%\" % ('=' * int(20. * (i + 1) / n - 1) + '>',\n",
    "                                                    int(100. * (i + 1) / n)))\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.flush()\n",
    "    data, label = np.array(data).swapaxes(1, 3), np.array(label).reshape((n, img_h * img_w, n_labels))\n",
    "\n",
    "    print(mode + ': OK')\n",
    "    print('\\tshapes: {}, {}'.format(data.shape, label.shape))\n",
    "    print('\\ttypes:  {}, {}'.format(data.dtype, label.dtype))\n",
    "    print('\\tmemory: {}, {} MB'.format(data.nbytes / 1048576, label.nbytes / 1048576))\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_results_2(output, indx):\n",
    "    df = pd.read_csv(path + 'test.csv')\n",
    "    \n",
    "    n_test_imgs = indx.shape[0]\n",
    "    plt.figure(figsize=(15, 2 * n_test_imgs))\n",
    "    \n",
    "    subplot_indx = 0\n",
    "    for cur_indx in indx:\n",
    "        for i, item in df.iterrows():\n",
    "            if i == cur_indx:\n",
    "                plt.subplot(n_test_imgs, 5, 5 * subplot_indx + 1)\n",
    "                plt.title('Input Image')\n",
    "                plt.axis('off')\n",
    "                im = resize(imread(path + item[0]), (img_h, img_w))\n",
    "                plt.imshow(im, cmap='gray')\n",
    "\n",
    "                plt.subplot(n_test_imgs, 5, 5 * subplot_indx + 2)\n",
    "                plt.title('Ground Truth')\n",
    "                plt.axis('off')\n",
    "                gt = resize(np.clip(~(imread(path + item[1])[:, :, 0]), 0, 1), (img_h, img_w))\n",
    "                plt.imshow(np.clip(gt, 0, 1), cmap='gray')\n",
    "\n",
    "                plt.subplot(n_test_imgs, 5, 5 * subplot_indx + 3)\n",
    "                plt.title('Prediction')\n",
    "                plt.axis('off')\n",
    "                labeled = np.argmax(output[subplot_indx], axis=-1)\n",
    "                plt.imshow(labeled, cmap='gray')\n",
    "\n",
    "                plt.subplot(n_test_imgs, 5, 5 * subplot_indx + 4)\n",
    "                plt.title('Heat map')\n",
    "                plt.axis('off')\n",
    "                plt.imshow(output[subplot_indx][:, :, 1])\n",
    "\n",
    "                plt.subplot(n_test_imgs, 5, 5 * subplot_indx + 5)\n",
    "                plt.title('Comparison')\n",
    "                plt.axis('off')\n",
    "                rgb = np.empty((img_h, img_w, 3))\n",
    "                rgb[:, :, 0] = labeled\n",
    "                img = resize(imread(path + item[0]), (img_h, img_w))\n",
    "                rgb[:, :, 1] = img\n",
    "                rgb[:, :, 2] = gt\n",
    "                plt.imshow(rgb)\n",
    "\n",
    "                subplot_indx += 1\n",
    "\n",
    "                break\n",
    "\n",
    "#     plt.savefig('result.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...done.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('model_5l.json'):\n",
    "    print('Loading model...', end='')\n",
    "    with open('model_5l.json', 'r') as f:\n",
    "        json_string = f.read()\n",
    "    autoencoder = model_from_json(json_string)\n",
    "    print('done.')\n",
    "else:\n",
    "    print('Defining model...', end='')\n",
    "    encoding_layers = [\n",
    "        Conv2D(64, (kernel, kernel), padding='same', input_shape=(img_h, img_w, 1)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(64, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Conv2D(128, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(128, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Conv2D(256, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(256, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(256, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(),\n",
    "    ]\n",
    "\n",
    "    autoencoder = models.Sequential()\n",
    "    autoencoder.encoding_layers = encoding_layers\n",
    "\n",
    "    for l in autoencoder.encoding_layers:\n",
    "        autoencoder.add(l)\n",
    "\n",
    "    decoding_layers = [\n",
    "        UpSampling2D(),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(512, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(256, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Conv2D(256, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(256, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(128, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Conv2D(128, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(64, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "\n",
    "        UpSampling2D(),\n",
    "        Conv2D(64, (kernel, kernel), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(n_labels, (1, 1), padding='valid'),\n",
    "        BatchNormalization(),\n",
    "    ]\n",
    "    autoencoder.decoding_layers = decoding_layers\n",
    "    for l in autoencoder.decoding_layers:\n",
    "        autoencoder.add(l)\n",
    "\n",
    "    autoencoder.add(Reshape((n_labels, img_h * img_w)))\n",
    "    autoencoder.add(Permute((2, 1)))\n",
    "    autoencoder.add(Activation('softmax'))\n",
    "\n",
    "    with open('model_5l.json', 'w') as outfile:\n",
    "        outfile.write(json.dumps(json.loads(autoencoder.to_json()), indent=2))\n",
    "    \n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 16, 16, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 128, 128, 2)       130       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 128, 128, 2)       8         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 16384)          0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 16384, 2)          0         \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16384, 2)          0         \n",
      "=================================================================\n",
      "Total params: 29,457,866.0\n",
      "Trainable params: 29,441,990.0\n",
      "Non-trainable params: 15,876.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and compiling model built with *'build_model.py'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...done.\n"
     ]
    }
   ],
   "source": [
    "print('Compiling model...', end='')\n",
    "# optimizer = SGD(lr=0.001, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "optimizer = Adam(lr=0.001, decay=0.0005)\n",
    "autoencoder.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...done.\n",
      "Loading testing data...done.\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'train_data_abcde.npy'\n",
    "train_label_path = 'train_label_abcde.npy'\n",
    "test_data_path = 'test_data_abcde.npy'\n",
    "test_label_path = 'test_label_abcde.npy'\n",
    "\n",
    "if os.path.isfile(train_data_path) and os.path.isfile(train_label_path):\n",
    "    print('Loading training data...', end='')\n",
    "    train_data = np.load(train_data_path)\n",
    "    train_label = np.load(train_label_path)\n",
    "    print('done.')\n",
    "else:\n",
    "    train_data, train_label = prep_data('train')\n",
    "    np.save(train_data_path, train_data)\n",
    "    np.save(train_label_path, train_label)\n",
    "\n",
    "if os.path.isfile(test_data_path) and os.path.isfile(test_label_path):\n",
    "    print('Loading testing data...', end='')\n",
    "    test_data = np.load(test_data_path)\n",
    "    test_label = np.load(test_label_path)\n",
    "    print('done.')\n",
    "else:\n",
    "    test_data, test_label = prep_data('test')\n",
    "    np.save(test_data_path, test_data)\n",
    "    np.save(test_label_path, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model or load existing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12250 samples, validate on 5250 samples\n",
      "Epoch 1/20\n",
      "12250/12250 [==============================] - 405s - loss: 0.1238 - acc: 0.9610 - val_loss: 0.0778 - val_acc: 0.9745\n",
      "Epoch 2/20\n",
      "12250/12250 [==============================] - 385s - loss: 0.0604 - acc: 0.9797 - val_loss: 0.0542 - val_acc: 0.9805\n",
      "Epoch 3/20\n",
      "12250/12250 [==============================] - 386s - loss: 0.0464 - acc: 0.9831 - val_loss: 0.0474 - val_acc: 0.9816\n",
      "Epoch 4/20\n",
      "12250/12250 [==============================] - 383s - loss: 0.0392 - acc: 0.9849 - val_loss: 0.0369 - val_acc: 0.9857\n",
      "Epoch 5/20\n",
      "12250/12250 [==============================] - 383s - loss: 0.0338 - acc: 0.9866 - val_loss: 0.0314 - val_acc: 0.9876\n",
      "Epoch 6/20\n",
      "12250/12250 [==============================] - 375s - loss: 0.0302 - acc: 0.9879 - val_loss: 0.0285 - val_acc: 0.9887\n",
      "Epoch 7/20\n",
      "12250/12250 [==============================] - 374s - loss: 0.0268 - acc: 0.9891 - val_loss: 0.0258 - val_acc: 0.9895\n",
      "Epoch 8/20\n",
      "12250/12250 [==============================] - 362s - loss: 0.0247 - acc: 0.9899 - val_loss: 0.0239 - val_acc: 0.9902\n",
      "Epoch 9/20\n",
      "12250/12250 [==============================] - 363s - loss: 0.0228 - acc: 0.9906 - val_loss: 0.0227 - val_acc: 0.9905\n",
      "Epoch 10/20\n",
      "12250/12250 [==============================] - 364s - loss: 0.0215 - acc: 0.9912 - val_loss: 0.0214 - val_acc: 0.9911\n",
      "Epoch 11/20\n",
      "12250/12250 [==============================] - 369s - loss: 0.0204 - acc: 0.9916 - val_loss: 0.0199 - val_acc: 0.9919\n",
      "Epoch 12/20\n",
      " 7272/12250 [================>.............] - ETA: 134s - loss: 0.0193 - acc: 0.9921"
     ]
    }
   ],
   "source": [
    "if TRAIN == 1:\n",
    "    nb_epoch = 20\n",
    "    batch_size = 8\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=5)\n",
    "\n",
    "    history = autoencoder.fit(train_data, train_label,\n",
    "                              batch_size=batch_size, epochs=nb_epoch,\n",
    "                              verbose=1,\n",
    "                              validation_data=(test_data, test_label),\n",
    "                              callbacks=[early_stopping])\n",
    "\n",
    "    # Save trained model\n",
    "    autoencoder.save_weights('model_5l_weight_ep50.hdf5')\n",
    "    \n",
    "    # Save training history\n",
    "    with open('training_his.pickle', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "else:\n",
    "    # Load trained model\n",
    "    autoencoder.load_weights('model_5l_weight_ep50.hdf5')\n",
    "    \n",
    "    # Load training history\n",
    "    file = open('training_his.pickle', 'rb')\n",
    "    history_ = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_model(autoencoder, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testing data and evaluate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = autoencoder.evaluate(test_data, test_label, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# score = autoencoder.evaluate(train_data, train_label, verbose=0)\n",
    "# print('Train score:', score[0])\n",
    "# print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testing_indx = np.random.choice(n_test, 5)\n",
    "\n",
    "test_ite = test_data[testing_indx, :, :, :]\n",
    "# test_ite = np.expand_dims(test_ite, axis=0)\n",
    "\n",
    "output = autoencoder.predict_proba(test_ite, verbose=0)\n",
    "output = output.reshape((output.shape[0], img_h, img_w, n_labels))\n",
    "\n",
    "plot_results_2(output, testing_indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realtime visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "n_test_imgs = 1\n",
    "n_type_outputs = 3\n",
    "\n",
    "for i, item in df.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Call trained model\n",
    "    test_ite = test_data[i, :, :, :]\n",
    "    test_ite = np.expand_dims(test_ite, axis=0)\n",
    "    start_time = time.time()\n",
    "    output = autoencoder.predict_proba(test_ite, verbose=0)\n",
    "    pred_time = time.time() - start_time\n",
    "    output = output.reshape((output.shape[0], img_h, img_w, n_labels))\n",
    "    \n",
    "    # Load input image\n",
    "    im = resize(imread(path + item[0]), (img_h, img_w))\n",
    "\n",
    "    #\n",
    "    rgb = np.empty((img_h, img_w, 3))\n",
    "    labeled = np.argmax(output[0], axis=-1)\n",
    "    rgb[:, :, 0] = img\n",
    "    img = resize(imread(path + item[0]), (img_h, img_w))\n",
    "    rgb[:, :, 1] = img + labeled\n",
    "    rgb[:, :, 2] = img\n",
    "\n",
    "    rgb = cv2.resize(rgb, (256, 213))\n",
    "    \n",
    "    cv2.imshow('hand segmentation', rgb)\n",
    "    print('FPS: ' + str(int(1.0/pred_time)))\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:     # Esc\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history_.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_['acc'])\n",
    "plt.plot(history_['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_['loss'])\n",
    "plt.plot(history_['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_results(output):\n",
    "#     gt = []\n",
    "#     df = pd.read_csv(path + 'test.csv')\n",
    "#     for i, item in df.iterrows():\n",
    "#         gt.append(np.clip(imread(path + item[1]), 0, 1))\n",
    "\n",
    "#     plt.figure(figsize=(15, 2 * n_test))\n",
    "#     for i, item in df.iterrows():\n",
    "#         if i > 3:\n",
    "#             break\n",
    "#         plt.subplot(n_test, 5, 5 * i + 1)\n",
    "#         plt.title('Input Image')\n",
    "#         plt.axis('off')\n",
    "# #         im = imread(path + item[0])\n",
    "#         im = resize(imread(path + item[0]), (img_h, img_w))\n",
    "#         plt.imshow(im, cmap='gray')\n",
    "        \n",
    "#         plt.subplot(n_test, 5, 5 * i + 2)\n",
    "#         plt.title('Ground Truth')\n",
    "#         plt.axis('off')\n",
    "# #         gt = imread(path + item[1])\n",
    "#         gt = resize(np.clip(~(imread(path + item[1])[:, :, 0]), 0, 1), (img_h, img_w))\n",
    "#         plt.imshow(np.clip(gt, 0, 1), cmap='gray')\n",
    "\n",
    "#         plt.subplot(n_test, 5, 5 * i + 3)\n",
    "#         plt.title('Prediction')\n",
    "#         plt.axis('off')\n",
    "#         labeled = np.argmax(output[i], axis=-1)\n",
    "#         plt.imshow(labeled, cmap='gray')\n",
    "\n",
    "#         plt.subplot(n_test, 5, 5 * i + 4)\n",
    "#         plt.title('Heat map')\n",
    "#         plt.axis('off')\n",
    "#         plt.imshow(output[i][:, :, 1])\n",
    "\n",
    "#         plt.subplot(n_test, 5, 5 * i + 5)\n",
    "#         plt.title('Comparison')\n",
    "#         plt.axis('off')\n",
    "#         rgb = np.empty((img_h, img_w, 3))\n",
    "#         rgb[:, :, 0] = labeled\n",
    "#         img = resize(imread(path + item[0]), (img_h, img_w))\n",
    "#         rgb[:, :, 1] = img\n",
    "#         rgb[:, :, 2] = gt\n",
    "#         plt.imshow(rgb)\n",
    "\n",
    "#     plt.savefig('result.png')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
